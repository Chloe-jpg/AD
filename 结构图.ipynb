{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model\n",
    "from keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Deconvolution3D\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "\n",
    "try:\n",
    "    from keras.engine import merge\n",
    "except ImportError:\n",
    "    from keras.layers.merge import concatenate\n",
    "\n",
    "\n",
    "def unet_model_3d(input_shape, pool_size=(2, 2, 2), n_labels=1, deconvolution=False,\n",
    "                  depth=4, n_base_filters=32, batch_normalization=False, activation_name=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Builds the 3D UNet Keras model.f\n",
    "    :param metrics: List metrics to be calculated during model training (default is dice coefficient).\n",
    "    :param include_label_wise_dice_coefficients: If True and n_labels is greater than 1, model will report the dice\n",
    "    coefficient for each label as metric.\n",
    "    :param n_base_filters: The number of filters that the first layer in the convolution network will have. Following\n",
    "    layers will contain a multiple of this number. Lowering this number will likely reduce the amount of memory required\n",
    "    to train the model.\n",
    "    :param depth: indicates the depth of the U-shape for the model. The greater the depth, the more max pooling\n",
    "    layers will be added to the model. Lowering the depth may reduce the amount of memory required for training.\n",
    "    :param input_shape: Shape of the input data (n_chanels, x_size, y_size, z_size). The x, y, and z sizes must be\n",
    "    divisible by the pool size to the power of the depth of the UNet, that is pool_size^depth.\n",
    "    :param pool_size: Pool size for the max pooling operations.\n",
    "    :param n_labels: Number of binary labels that the model is learning.\n",
    "    :param initial_learning_rate: Initial learning rate for the model. This will be decayed during training.\n",
    "    :param deconvolution: If set to True, will use transpose convolution(deconvolution) instead of up-sampling. This\n",
    "    increases the amount memory required during training.\n",
    "    :return: Untrained 3D UNet Model\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    current_layer = inputs\n",
    "    levels = list()\n",
    "    num_layer = 0\n",
    "\n",
    "    # add levels with max pooling\n",
    "    for layer_depth in range(depth):\n",
    "        layer1 = create_convolution_block(input_layer=current_layer, n_filters=n_base_filters*(2**layer_depth),\n",
    "                                          batch_normalization=batch_normalization, layer_depth=num_layer)\n",
    "        num_layer += 1\n",
    "        layer2 = create_convolution_block(input_layer=layer1, n_filters=n_base_filters*(2**layer_depth)*2,\n",
    "                                          batch_normalization=batch_normalization, layer_depth=num_layer)\n",
    "        num_layer += 1\n",
    "        if layer_depth < depth - 1:\n",
    "            current_layer = MaxPooling3D(pool_size=pool_size)(layer2)\n",
    "            levels.append([layer1, layer2, current_layer])\n",
    "        else:\n",
    "            current_layer = layer2\n",
    "            levels.append([layer1, layer2])\n",
    "\n",
    "    # add levels with up-convolution or up-sampling\n",
    "    for layer_depth in range(depth-2, -1, -1):\n",
    "        up_convolution = get_up_convolution(pool_size=pool_size, deconvolution=deconvolution,\n",
    "                                            n_filters=current_layer._keras_shape[1])(current_layer)\n",
    "        concat = concatenate([up_convolution, levels[layer_depth][1]], axis=1)\n",
    "        current_layer = create_convolution_block(n_filters=levels[layer_depth][1]._keras_shape[1], layer_depth=num_layer,\n",
    "                                                 input_layer=concat, batch_normalization=batch_normalization)\n",
    "        num_layer += 1\n",
    "        current_layer = create_convolution_block(n_filters=levels[layer_depth][1]._keras_shape[1], layer_depth=num_layer,\n",
    "                                                 input_layer=current_layer,\n",
    "                                                 batch_normalization=batch_normalization)\n",
    "        num_layer += 1\n",
    "\n",
    "    final_convolution = Conv3D(n_labels, (1, 1, 1))(current_layer)\n",
    "    act = Activation(activation_name)(final_convolution)\n",
    "    model = Model(inputs=inputs, outputs=act)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_convolution_block(input_layer, n_filters, batch_normalization=False, kernel=(3, 3, 3), activation=None,\n",
    "                             padding='same', strides=(1, 1, 1), instance_normalization=False, layer_depth=None):\n",
    "    \"\"\"\n",
    "    :param strides:\n",
    "    :param input_layer:\n",
    "    :param n_filters:\n",
    "    :param batch_normalization:\n",
    "    :param kernel:\n",
    "    :param activation: Keras activation layer to use. (default is 'relu')\n",
    "    :param padding:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    layer = Conv3D(n_filters, kernel, padding=padding, strides=strides, name=\"depth_\"+str(layer_depth)+\"_conv\")(input_layer)\n",
    "    if batch_normalization:\n",
    "        layer = BatchNormalization(axis=1, name=\"depth_\"+str(layer_depth)+\"_bn\")(layer)\n",
    "    elif instance_normalization:\n",
    "        try:\n",
    "            from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Install keras_contrib in order to use instance normalization.\"\n",
    "                              \"\\nTry: pip install git+https://www.github.com/farizrahman4u/keras-contrib.git\")\n",
    "        layer = InstanceNormalization(axis=1, name=\"depth_\"+str(layer_depth)+\"_in\")(layer)\n",
    "    if activation is None:\n",
    "        return Activation('relu', name=\"depth_\"+str(layer_depth)+\"_relu\")(layer)\n",
    "    else:\n",
    "        return activation()(layer)\n",
    "\n",
    "\n",
    "def compute_level_output_shape(n_filters, depth, pool_size, image_shape):\n",
    "    \"\"\"\n",
    "    Each level has a particular output shape based on the number of filters used in that level and the depth or number \n",
    "    of max pooling operations that have been done on the data at that point.\n",
    "    :param image_shape: shape of the 3d image.\n",
    "    :param pool_size: the pool_size parameter used in the max pooling operation.\n",
    "    :param n_filters: Number of filters used by the last node in a given level.\n",
    "    :param depth: The number of levels down in the U-shaped model a given node is.\n",
    "    :return: 5D vector of the shape of the output node \n",
    "    \"\"\"\n",
    "    output_image_shape = np.asarray(np.divide(image_shape, np.power(pool_size, depth)), dtype=np.int32).tolist()\n",
    "    return tuple([None, n_filters] + output_image_shape)\n",
    "\n",
    "\n",
    "def get_up_convolution(n_filters, pool_size, kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
    "                       deconvolution=False):\n",
    "    if deconvolution:\n",
    "        return Deconvolution3D(filters=n_filters, kernel_size=kernel_size,\n",
    "                               strides=strides)\n",
    "    else:\n",
    "        return UpSampling3D(size=pool_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import  f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc, roc_auc_score, confusion_matrix as CM\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import * \n",
    "from keras.optimizers import Adam \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, GlobalAveragePooling3D, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cretae the binary input \n",
    "def data_filter(data, label, exclu):\n",
    "    idx = np.where(label!= exclu)[0]\n",
    "    #print(len(idx))\n",
    "    data_new = data[idx]\n",
    "    label_new = label[idx]\n",
    "    #print(data_new.shape)\n",
    "    print(np.unique(label_new, return_counts=True))\n",
    "    return data_new, label_new\n",
    "\n",
    "# onehot encode labels for binary classifications\n",
    "def onehot_bi(y):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    y_encoded = onehot_encoder.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "# onehot encode labels for 3-way classifications\n",
    "def onehot_tri(y):\n",
    "    from keras.utils import to_categorical\n",
    "    return to_categorical(y)\n",
    "\n",
    "# view the distribution of class labels of the input data\n",
    "def showpercentage(array):\n",
    "    pcn = array[1][0]/np.sum(array[1])\n",
    "    pmci = array[1][1]/np.sum(array[1])\n",
    "    pad = array[1][2]/np.sum(array[1])\n",
    "    print(str(pad) + \" percent of the data has AD label\")\n",
    "    print(str(pcn) + \" percent of the data has CN label\")\n",
    "    print(str(pmci) + \" percent of the data has MCI label\")\n",
    "# visualizatio of model traning and model performance \n",
    "\n",
    "# visualize the training and validation performance\n",
    "def plot_history(data_list, label_list, title, ylabel, name):\n",
    "\n",
    "    epochs = range(1, len(data_list[0]) + 1)\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        plt.plot(epochs, data, label=label)\n",
    "    plt.title(title, pad = 10, fontsize='large')\n",
    "    plt.xlabel('Epochs', labelpad=10)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#%%\n",
    "\n",
    "# model evaluation\n",
    "    \n",
    "# evaluate model performance - binary classifications\n",
    "def evaluate_binary(X_test, y_test, model, name):\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    test_y_prob = model.predict(X_test)\n",
    "    print(test_y_prob)\n",
    "    test_y_pred = np.argmax(test_y_prob, axis = 1)\n",
    "    test_y_true = np.argmax(y_test, axis = 1) \n",
    "    # accuracy\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    # AUC\n",
    "    pos_prob = test_y_prob[:,1]\n",
    "    auc_score = roc_auc_score(test_y_true, pos_prob)\n",
    "    # precision, recall, specificity, and f1_score\n",
    "    p = precision_score(test_y_true, test_y_pred)\n",
    "    r = recall_score(test_y_true, test_y_pred)\n",
    "    f1 = f1_score(test_y_true, test_y_pred)\n",
    "#     sen, spe, _ = sss(test_y_true, test_y_pred, average=\"binary\")\n",
    "    print(test_y_true, test_y_pred)\n",
    "    # print results\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    print(\"Test AUC is: \", auc_score)\n",
    "    print(\"Test confusion matrix: \\n\", CM(test_y_true, test_y_pred))\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "#     print(\"Specificity: \", spe)\n",
    "    print(\"f1_score: \", f1)\n",
    "\n",
    "    # plot and save roc curve\n",
    "    pos_prob = test_y_prob[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(test_y_true, pos_prob)\n",
    "    ns_probs = [0 for _ in range(len(test_y_prob))]\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(test_y_true, ns_probs)\n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.plot(fpr,tpr, marker = '.', color = 'darkorange', label = 'Model AUC (area = {:.2f})'.format(auc_score)) \n",
    "    plt.plot(ns_fpr, ns_tpr, color = 'royalblue', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "# evaluate model performance - 3 way classifiations\n",
    "def evaluate_3way(X_test, y_test, model):\n",
    "    test_y_prob = model.predict(X_test)\n",
    "    test_y_pred = np.argmax(test_y_prob, axis = 1)\n",
    "    test_y_true = np.argmax(y_test, axis = 1) \n",
    "    print(test_y_prob)\n",
    "    # accuracy\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    # precision, recall, specificity, and f1_score\n",
    "    p = precision_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "    r = recall_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "    f1 = f1_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "#     sen,spe,_ = sss(test_y_true, test_y_pred, average=\"macro\")\n",
    "    print(test_y_true, test_y_pred)\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    print(\"Test confusion matrix: \\n\", CM(test_y_true, test_y_pred))\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "#     print(\"Specificity: \", spe)\n",
    "    print(\"f1_score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the Genesis model(set %tensorflow_version 1.x for training Genesis)\n",
    "def run_genesis(X_train, y_train, X_valid = None, y_valid = None, \n",
    "             final = False, out = 2,\n",
    "             dr = 0.02, n_epochs = 150, batch_size = 15):\n",
    "  \n",
    "    input_channels, input_rows, input_cols, input_deps = 1, 64, 64, 64\n",
    "\n",
    "    weight_dir = 'Genesis_Chest_CT.h5'\n",
    "\n",
    "    models_genesis = unet_model_3d((input_channels, input_rows, input_cols, input_deps), batch_normalization=True)\n",
    "    plot_model(models_genesis, to_file='test2.png', show_shapes=True)\n",
    "    print(\"Load pre-trained Models Genesis weights from {}\".format(weight_dir))\n",
    "    models_genesis.load_weights(weight_dir)\n",
    "\n",
    "    x = models_genesis.get_layer('depth_7_relu').output\n",
    "    x = GlobalAveragePooling3D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(dr)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dr)(x)\n",
    "\n",
    "    output = Dense(out, activation = 'softmax')(x)\n",
    "\n",
    "    # model optimization\n",
    "    # opt = Adam(learning_rate = lr)\n",
    "    model = keras.models.Model(inputs=models_genesis.input, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.5, patience = 5, \n",
    "                         verbose = 1, mode = 'min')\n",
    "\n",
    "\n",
    "    # model training and fine-tuning\n",
    "    if not final:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "#             plot_model(model, to_file='test.png', show_shapes=True)\n",
    "            hist = model.fit(X_train, y_train,\n",
    "                             batch_size = batch_size, \n",
    "                             epochs = n_epochs,\n",
    "                             callbacks=[cb],\n",
    "                             validation_data = (X_valid, y_valid), \n",
    "                             shuffle = True)\n",
    "            evaluate_3way(X_valid, y_valid, model)\n",
    "            return model, hist\n",
    "\n",
    "    # model final training for testing (train + valid combined)\n",
    "    else:\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            hist = model.fit(X_train, y_train,\n",
    "                      batch_size = batch_size, \n",
    "                      epochs = n_epochs,\n",
    "                      callbacks=[cb],\n",
    "                      shuffle = True)\n",
    "\n",
    "\n",
    "            return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356, 64, 64, 64)\n",
      "(356,)\n",
      "(86, 64, 64, 64)\n",
      "(86,)\n",
      "(90, 64, 64, 64)\n",
      "(90,)\n",
      "Train:\n",
      "0.2893258426966292 percent of the data has AD label\n",
      "0.41853932584269665 percent of the data has CN label\n",
      "0.29213483146067415 percent of the data has MCI label\n",
      "\n",
      "Validation:\n",
      "0.28888888888888886 percent of the data has AD label\n",
      "0.4222222222222222 percent of the data has CN label\n",
      "0.28888888888888886 percent of the data has MCI label\n",
      "\n",
      "Test\n",
      "0.29069767441860467 percent of the data has AD label\n",
      "0.4186046511627907 percent of the data has CN label\n",
      "0.29069767441860467 percent of the data has MCI label\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  Load in the input - original data \"\"\"\n",
    "\n",
    "Xtr = np.load(\"preprocess/input/random_split/train_data.npy\", allow_pickle = True)\n",
    "ytr = np.load(\"preprocess/input/random_split/train_label.npy\", allow_pickle = True)\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "\n",
    "Xts = np.load(\"preprocess/input/random_split/test_data.npy\", allow_pickle = True)\n",
    "yts = np.load(\"preprocess/input/random_split/test_label.npy\", allow_pickle = True)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n",
    "\n",
    "Xval = np.load(\"preprocess/input/random_split/val_data.npy\", allow_pickle = True)\n",
    "yval = np.load(\"preprocess/input/random_split/val_label.npy\", allow_pickle = True)\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "\n",
    "print(\"Train:\")\n",
    "showpercentage(np.unique(ytr, return_counts=True))\n",
    "print()\n",
    "print(\"Validation:\")\n",
    "showpercentage(np.unique(yval, return_counts=True))\n",
    "print()\n",
    "print(\"Test\")\n",
    "showpercentage(np.unique(yts, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 2]), array([149, 103]))\n",
      "(array([0, 2]), array([38, 26]))\n",
      "(array([0, 2]), array([36, 25]))\n",
      "Load pre-trained Models Genesis weights from Genesis_Chest_CT.h5\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 252 samples, validate on 64 samples\n",
      "Epoch 1/150\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  NC vs. AD - transfer learning \"\"\"\n",
    "\n",
    "# create input for binary classification of NC vs. AD\n",
    "Xtr_ncad, ytr_ncad = data_filter(Xtr, ytr, 1)\n",
    "Xval_ncad, yval_ncad = data_filter(Xval, yval, 1)\n",
    "Xts_ncad, yts_ncad = data_filter(Xts, yts, 1)\n",
    "\n",
    "# reshape the input\n",
    "X_train = np.transpose(Xtr_ncad.reshape(-1,64,64,64,1), (0, 4, 1, 2, 3)) \n",
    "X_test = np.transpose(Xts_ncad.reshape(-1,64,64,64,1), (0, 4, 1, 2, 3))\n",
    "X_val = np.transpose(Xval_ncad.reshape(-1,64,64,64,1), (0, 4, 1, 2, 3))\n",
    "\n",
    "# one hot encode the target labels \n",
    "y_train = onehot_bi(ytr_ncad)\n",
    "y_test = onehot_bi(yts_ncad)\n",
    "y_val = onehot_bi(yval_ncad)\n",
    "\n",
    "\n",
    "# model training\n",
    "model, hist = run_genesis(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from imblearn.metrics import sensitivity_specificity_support as sss\n",
    "from sklearn.metrics import  f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc, roc_auc_score, confusion_matrix as CM\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, Conv3D, GlobalAveragePooling3D,  MaxPooling3D, LeakyReLU, BatchNormalization, Dropout, Flatten, Activation, Reshape,  Conv3DTranspose, UpSampling3D\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cretae the binary input \n",
    "def data_filter(data, label, exclu):\n",
    "    idx = np.where(label!= exclu)[0]\n",
    "    #print(len(idx))\n",
    "    data_new = data[idx]\n",
    "    label_new = label[idx]\n",
    "    #print(data_new.shape)\n",
    "    print(np.unique(label_new, return_counts=True))\n",
    "    return data_new, label_new\n",
    "\n",
    "# onehot encode labels for binary classifications\n",
    "def onehot_bi(y):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    y_encoded = onehot_encoder.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "# onehot encode labels for 3-way classifications\n",
    "def onehot_tri(y):\n",
    "    from keras.utils import to_categorical\n",
    "    return to_categorical(y)\n",
    "\n",
    "# view the distribution of class labels of the input data\n",
    "def showpercentage(array):\n",
    "    pcn = array[1][0]/np.sum(array[1])\n",
    "    pmci = array[1][1]/np.sum(array[1])\n",
    "    pad = array[1][2]/np.sum(array[1])\n",
    "    print(str(pad) + \" percent of the data has AD label\")\n",
    "    print(str(pcn) + \" percent of the data has CN label\")\n",
    "    print(str(pmci) + \" percent of the data has MCI label\")\n",
    "\n",
    "\n",
    "#%%\n",
    "# models\n",
    "\n",
    "# build the baseline model\n",
    "def run_base(X_train, y_train, X_valid = None, y_valid = None, \n",
    "             final = False, out = 2,\n",
    "             dr = 0.02, lr = 0.00001, \n",
    "             breg = l2(0.0001), areg = None, \n",
    "             n_epochs = 30, batch_size = 15):\n",
    "  \n",
    "    dim = (64, 64, 64, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32, kernel_size=(5,5,5),  kernel_initializer='he_uniform', bias_regularizer=breg, input_shape=dim))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(Conv3D(64, kernel_size=(5,5,5),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(5,5,5),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Dense(256, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(out, activation='softmax', activity_regularizer=areg))\n",
    "\n",
    "    # model optimization\n",
    "    opt = Adam(lr = lr)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.5, patience = 5, \n",
    "                         verbose = 1, epsilon = 1e-4, mode = 'min')\n",
    "    \n",
    "  # model training and fine-tuning\n",
    "    if not final:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                     batch_size = batch_size, \n",
    "                     epochs = n_epochs,\n",
    "                     callbacks=[cb],\n",
    "                     validation_data = (X_valid, y_valid), \n",
    "                     shuffle = True)\n",
    "  \n",
    "  # model final training for testing (train + valid combined)\n",
    "    else:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                  batch_size = batch_size, \n",
    "                  epochs = n_epochs,\n",
    "                  callbacks=[cb],\n",
    "                  shuffle = True)\n",
    "\n",
    "\n",
    "    return model, hist\n",
    "\n",
    "\n",
    "# build the convolutional autoencoder\n",
    "def run_cae(X_train,  X_val, lr = 0.0001, \n",
    "        n_epochs = 100, batch_size = 10):\n",
    "  \n",
    "    dim = (64,64,64,1)\n",
    "\n",
    "    inp = Input(dim)\n",
    "    # Encoder\n",
    "    e = Conv3D(32, (3, 3, 3), strides = 2, activation='elu',  kernel_initializer='he_uniform', padding = \"same\")(inp)\n",
    "    e = Conv3D(64, (3, 3, 3), strides = 2, activation='elu',  kernel_initializer='he_uniform', padding = \"same\")(e)\n",
    "    e = Conv3D(1, (3, 3, 3), strides = 2,activation='elu',  kernel_initializer='he_uniform', padding = \"same\", name = \"bottleneck\")(e)\n",
    "\n",
    "    #DECODER\n",
    "    d = Conv3DTranspose(64,(3,3,3), strides = 2, kernel_initializer='he_uniform', activation='elu', padding = \"same\")(e)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Conv3DTranspose(16,(3,3,3), strides=2,  kernel_initializer='he_uniform', activation='elu', padding = \"same\")(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Conv3DTranspose(16,(3,3,3), strides=2,  kernel_initializer='he_uniform', activation='elu', padding = \"same\")(d)\n",
    "    decoded = Conv3D(1, (3, 3, 3), activation='sigmoid', padding='same')(d)\n",
    "\n",
    "    ae = Model(inp, decoded)\n",
    "    \n",
    "    # model optimization\n",
    "    opt_ae = Adam(lr = lr)\n",
    "    ae.compile(optimizer = opt_ae, loss = \"mse\")\n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.9, patience = 3, \n",
    "                         verbose = 1, mode = 'min')\n",
    "    plot_model(ae, to_file='test3.png', show_shapes=True)\n",
    "    #Train it by providing training images\n",
    "    ae.fit(X_train, X_train, \n",
    "         batch_size = batch_size, \n",
    "         epochs = n_epochs, \n",
    "         validation_data = (X_val, X_val), \n",
    "         verbose = 1,\n",
    "         callbacks = [cb])\n",
    "\n",
    "    return ae\n",
    "\n",
    "# build the adapted baseline model for training with dimension reduced data\n",
    "def run_adpbase(X_train, y_train, X_valid = None, y_valid = None, \n",
    "             final = False, out = 2,\n",
    "             dr = 0.2, lr = 0.0001, \n",
    "             breg = l2(0.00001), wreg = l2(0.00001), areg = l1(0.00001), \n",
    "             n_epochs = 100, batch_size = 25):\n",
    "  \n",
    "    dim = (8,8,8,1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32, kernel_size=(2,2,2), kernel_initializer='he_uniform',bias_regularizer=breg, kernel_regularizer=wreg,input_shape=dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv3D(64, kernel_size=(2,2,2), kernel_initializer='he_uniform',bias_regularizer=breg, kernel_regularizer=wreg,input_shape=dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(2,2,2), kernel_initializer='he_uniform',bias_regularizer=breg, kernel_regularizer=wreg,input_shape=dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,kernel_initializer='he_uniform', kernel_regularizer=wreg,bias_regularizer=breg))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Dense(out, activation='softmax', activity_regularizer=areg))\n",
    "\n",
    "    # model optimization\n",
    "    opt = Adam(lr=lr)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.5, patience = 5, \n",
    "                         verbose = 1, epsilon = 1e-4, mode = 'min')\n",
    "    plot_model(model, to_file='test4.png', show_shapes=True)\n",
    "    # model training and fine-tuning\n",
    "    if not final:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                         batch_size = batch_size, \n",
    "                         epochs = n_epochs,\n",
    "                         callbacks=[cb],\n",
    "                         validation_data = (X_valid, y_valid), \n",
    "                         shuffle = True)\n",
    "\n",
    "    # model final training for testing (train + valid combined)\n",
    "    else:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                  batch_size = batch_size, \n",
    "                  epochs = n_epochs,\n",
    "                  callbacks = [cb],\n",
    "                  shuffle = True)\n",
    "\n",
    "\n",
    "    return model, hist\n",
    "\n",
    "#%%\n",
    "  \n",
    "# visualizatio of model traning and model performance \n",
    "\n",
    "# visualize the training and validation performance\n",
    "def plot_history(data_list, label_list, title, ylabel, name):\n",
    "\n",
    "    epochs = range(1, len(data_list[0]) + 1)\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        plt.plot(epochs, data, label=label)\n",
    "    plt.title(title, pad = 10, fontsize='large')\n",
    "    plt.xlabel('Epochs', labelpad=10)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#%%\n",
    "\n",
    "# model evaluation\n",
    "    \n",
    "# evaluate model performance - binary classifications\n",
    "def evaluate_binary(X_test, y_test, model, name):\n",
    "    test_y_prob = model.predict(X_test)\n",
    "    print(test_y_prob)\n",
    "    test_y_pred = np.argmax(test_y_prob, axis = 1)\n",
    "    test_y_true = np.argmax(y_test, axis = 1) \n",
    "    # accuracy\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    # AUC\n",
    "    pos_prob = test_y_prob[:,1]\n",
    "    auc_score = roc_auc_score(test_y_true, pos_prob)\n",
    "    # precision, recall, specificity, and f1_score\n",
    "    p = precision_score(test_y_true, test_y_pred)\n",
    "    r = recall_score(test_y_true, test_y_pred)\n",
    "    f1 = f1_score(test_y_true, test_y_pred)\n",
    "#     sen, spe, _ = sss(test_y_true, test_y_pred, average=\"binary\")\n",
    "    print(test_y_true, test_y_pred)\n",
    "    # print results\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    print(\"Test AUC is: \", auc_score)\n",
    "    print(\"Test confusion matrix: \\n\", CM(test_y_true, test_y_pred))\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "#     print(\"Specificity: \", spe)\n",
    "    print(\"f1_score: \", f1)\n",
    "\n",
    "    # plot and save roc curve\n",
    "    pos_prob = test_y_prob[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(test_y_true, pos_prob)\n",
    "    ns_probs = [0 for _ in range(len(test_y_prob))]\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(test_y_true, ns_probs)\n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.plot(fpr,tpr, marker = '.', color = 'darkorange', label = 'Model AUC (area = {:.2f})'.format(auc_score)) \n",
    "    plt.plot(ns_fpr, ns_tpr, color = 'royalblue', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "# evaluate model performance - 3 way classifiations\n",
    "def evaluate_3way(X_test, y_test, model):\n",
    "    test_y_prob = model.predict(X_test)\n",
    "    test_y_pred = np.argmax(test_y_prob, axis = 1)\n",
    "    test_y_true = np.argmax(y_test, axis = 1) \n",
    "    print(test_y_prob)\n",
    "    # accuracy\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    # precision, recall, specificity, and f1_score\n",
    "    p = precision_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "    r = recall_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "    f1 = f1_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "#     sen,spe,_ = sss(test_y_true, test_y_pred, average=\"macro\")\n",
    "    print(test_y_true, test_y_pred)\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    print(\"Test confusion matrix: \\n\", CM(test_y_true, test_y_pred))\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "#     print(\"Specificity: \", spe)\n",
    "    print(\"f1_score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356, 64, 64, 64)\n",
      "(356,)\n",
      "(86, 64, 64, 64)\n",
      "(86,)\n",
      "(90, 64, 64, 64)\n",
      "(90,)\n",
      "Train:\n",
      "0.2893258426966292 percent of the data has AD label\n",
      "0.41853932584269665 percent of the data has CN label\n",
      "0.29213483146067415 percent of the data has MCI label\n",
      "\n",
      "Validation:\n",
      "0.28888888888888886 percent of the data has AD label\n",
      "0.4222222222222222 percent of the data has CN label\n",
      "0.28888888888888886 percent of the data has MCI label\n",
      "\n",
      "Test\n",
      "0.29069767441860467 percent of the data has AD label\n",
      "0.4186046511627907 percent of the data has CN label\n",
      "0.29069767441860467 percent of the data has MCI label\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  Load in the input - original data \"\"\"\n",
    "\n",
    "Xtr = np.load(\"preprocess/input/random_split/train_data.npy\", allow_pickle = True)\n",
    "ytr = np.load(\"preprocess/input/random_split/train_label.npy\", allow_pickle = True)\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "\n",
    "Xts = np.load(\"preprocess/input/random_split/test_data.npy\", allow_pickle = True)\n",
    "yts = np.load(\"preprocess/input/random_split/test_label.npy\", allow_pickle = True)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n",
    "\n",
    "Xval = np.load(\"preprocess/input/random_split/val_data.npy\", allow_pickle = True)\n",
    "yval = np.load(\"preprocess/input/random_split/val_label.npy\", allow_pickle = True)\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "\n",
    "print(\"Train:\")\n",
    "showpercentage(np.unique(ytr, return_counts=True))\n",
    "print()\n",
    "print(\"Validation:\")\n",
    "showpercentage(np.unique(yval, return_counts=True))\n",
    "print()\n",
    "print(\"Test\")\n",
    "showpercentage(np.unique(yts, return_counts=True))\n",
    "\n",
    "#%% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 356 samples, validate on 90 samples\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "356/356 [==============================] - 8s 23ms/sample - loss: 0.1626 - val_loss: 0.1546\n",
      "Epoch 2/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.1122 - val_loss: 0.1247\n",
      "Epoch 3/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0743 - val_loss: 0.0903\n",
      "Epoch 4/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0607 - val_loss: 0.0714\n",
      "Epoch 5/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0538 - val_loss: 0.0612\n",
      "Epoch 6/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0482 - val_loss: 0.0553\n",
      "Epoch 7/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0419 - val_loss: 0.0493\n",
      "Epoch 8/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0355 - val_loss: 0.0422\n",
      "Epoch 9/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0296 - val_loss: 0.0356\n",
      "Epoch 10/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0246 - val_loss: 0.0312\n",
      "Epoch 11/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0209 - val_loss: 0.0272\n",
      "Epoch 12/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0182 - val_loss: 0.0238\n",
      "Epoch 13/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0164 - val_loss: 0.0202\n",
      "Epoch 14/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0152 - val_loss: 0.0174\n",
      "Epoch 15/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0142 - val_loss: 0.0174\n",
      "Epoch 16/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0136 - val_loss: 0.0151\n",
      "Epoch 17/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0131 - val_loss: 0.0150\n",
      "Epoch 18/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 19/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0125 - val_loss: 0.0127\n",
      "Epoch 20/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0123 - val_loss: 0.0122\n",
      "Epoch 21/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0121 - val_loss: 0.0116\n",
      "Epoch 22/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0118 - val_loss: 0.0115\n",
      "Epoch 23/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0116 - val_loss: 0.0113\n",
      "Epoch 24/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0115 - val_loss: 0.0112\n",
      "Epoch 25/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0114 - val_loss: 0.0112\n",
      "Epoch 26/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0113 - val_loss: 0.0109\n",
      "Epoch 27/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0112 - val_loss: 0.0108\n",
      "Epoch 28/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 29/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 30/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0109\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 8.999999772640876e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0109 - val_loss: 0.0107\n",
      "Epoch 31/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 32/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0107 - val_loss: 0.0112\n",
      "Epoch 33/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 34/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0106 - val_loss: 0.0107\n",
      "Epoch 35/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 36/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0106\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 8.100000122794882e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0105 - val_loss: 0.0103\n",
      "Epoch 37/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 38/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0105 - val_loss: 0.0102\n",
      "Epoch 39/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0104 - val_loss: 0.0100\n",
      "Epoch 40/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 41/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0103 - val_loss: 0.0099\n",
      "Epoch 42/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0103\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 7.289999848580919e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 43/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0102 - val_loss: 0.0103\n",
      "Epoch 44/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 45/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0102 - val_loss: 0.0099\n",
      "Epoch 46/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0102 - val_loss: 0.0098\n",
      "Epoch 47/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0101\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 6.56100019114092e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 48/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 49/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0101 - val_loss: 0.0098\n",
      "Epoch 50/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0100\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 5.904900172026828e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 51/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 52/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0100 - val_loss: 0.0097\n",
      "Epoch 53/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0100 - val_loss: 0.0096\n",
      "Epoch 54/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0099\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 5.314410154824145e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 55/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 56/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 57/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 58/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 59/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0099\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 4.7829690083744934e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 60/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0096\n",
      "Epoch 61/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 62/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0098\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 4.304672074795235e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 63/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 64/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 65/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0098\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 3.8742047036066654e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 66/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 67/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0096\n",
      "Epoch 68/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 69/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 3.4867842987296176e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 70/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 71/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0098 - val_loss: 0.0095\n",
      "Epoch 72/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 3.138105967082083e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 73/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 74/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 75/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 2.824295370373875e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0098\n",
      "Epoch 76/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 77/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 78/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 2.5418658333364876e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 79/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 80/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 81/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 82/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0097\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 2.2876792172610294e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 83/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 84/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 85/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 2.0589113773894496e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0093\n",
      "Epoch 86/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 87/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 88/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.8530202396505047e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 89/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 90/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 91/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 1.667718133830931e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 92/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 93/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 94/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 1.5009462549642194e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 95/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 96/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 97/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 1.3508516622096067e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 98/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 99/100\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "Epoch 100/100\n",
      "350/356 [============================>.] - ETA: 0s - loss: 0.0096\n",
      "Epoch 00100: ReduceLROnPlateau reducing learning rate to 1.2157664878031938e-05.\n",
      "356/356 [==============================] - 4s 11ms/sample - loss: 0.0096 - val_loss: 0.0093\n",
      "(356, 8, 8, 8, 1)\n",
      "(90, 8, 8, 8, 1)\n",
      "(86, 8, 8, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Dimenstion reduction\"\"\"\n",
    "\n",
    "# reshape the input\n",
    "X_train = Xtr.reshape(-1,64,64,64,1) \n",
    "X_test = Xts.reshape(-1,64,64,64,1) \n",
    "X_val = Xval.reshape(-1,64,64,64,1) \n",
    "\n",
    "# traninng convolutional autoencoder(CAE)\n",
    "cae = run_cae(X_train, X_val)\n",
    "\n",
    "# use CAE to predict the train, validation and test data\n",
    "encoder = Model(cae.input, cae.get_layer('bottleneck').output)\n",
    "\n",
    "# predict\n",
    "Xtr_dr = encoder.predict(X_train)\n",
    "Xval_dr = encoder.predict(X_val)\n",
    "Xts_dr = encoder.predict(X_test)\n",
    "\n",
    "# check the new input shape\n",
    "print(Xtr_dr.shape)\n",
    "print(Xval_dr.shape)\n",
    "print(Xts_dr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 2]), array([149, 103]))\n",
      "(array([0, 2]), array([38, 26]))\n",
      "(array([0, 2]), array([36, 25]))\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 252 samples, validate on 64 samples\n",
      "Epoch 1/100\n",
      "252/252 [==============================] - 2s 7ms/sample - loss: 0.8057 - acc: 0.5992 - val_loss: 5.5651 - val_acc: 0.5938\n",
      "Epoch 2/100\n",
      "252/252 [==============================] - 0s 357us/sample - loss: 0.6368 - acc: 0.6905 - val_loss: 5.0209 - val_acc: 0.5938\n",
      "Epoch 3/100\n",
      "252/252 [==============================] - 0s 345us/sample - loss: 0.6473 - acc: 0.6865 - val_loss: 4.9133 - val_acc: 0.5938\n",
      "Epoch 4/100\n",
      "252/252 [==============================] - 0s 348us/sample - loss: 0.6373 - acc: 0.6746 - val_loss: 3.9582 - val_acc: 0.5938\n",
      "Epoch 5/100\n",
      "252/252 [==============================] - 0s 349us/sample - loss: 0.5627 - acc: 0.7222 - val_loss: 2.4922 - val_acc: 0.5938\n",
      "Epoch 6/100\n",
      "252/252 [==============================] - 0s 340us/sample - loss: 0.6250 - acc: 0.6587 - val_loss: 3.3023 - val_acc: 0.5938\n",
      "Epoch 7/100\n",
      "252/252 [==============================] - 0s 339us/sample - loss: 0.6189 - acc: 0.7063 - val_loss: 2.5337 - val_acc: 0.5938\n",
      "Epoch 8/100\n",
      "252/252 [==============================] - 0s 343us/sample - loss: 0.5405 - acc: 0.7421 - val_loss: 1.9978 - val_acc: 0.5938\n",
      "Epoch 9/100\n",
      "252/252 [==============================] - 0s 334us/sample - loss: 0.5902 - acc: 0.7024 - val_loss: 0.9979 - val_acc: 0.6094\n",
      "Epoch 10/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.5949 - acc: 0.7103 - val_loss: 1.2065 - val_acc: 0.5938\n",
      "Epoch 11/100\n",
      "252/252 [==============================] - 0s 336us/sample - loss: 0.5824 - acc: 0.7500 - val_loss: 1.8904 - val_acc: 0.5938\n",
      "Epoch 12/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.5237 - acc: 0.7619 - val_loss: 1.8364 - val_acc: 0.5938\n",
      "Epoch 13/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.6021 - acc: 0.7302 - val_loss: 0.8940 - val_acc: 0.6406\n",
      "Epoch 14/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.5726 - acc: 0.7183 - val_loss: 1.1884 - val_acc: 0.6250\n",
      "Epoch 15/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.5602 - acc: 0.7262 - val_loss: 1.1852 - val_acc: 0.6250\n",
      "Epoch 16/100\n",
      "252/252 [==============================] - 0s 344us/sample - loss: 0.5499 - acc: 0.7500 - val_loss: 0.8059 - val_acc: 0.7188\n",
      "Epoch 17/100\n",
      "252/252 [==============================] - 0s 341us/sample - loss: 0.5706 - acc: 0.7381 - val_loss: 0.6482 - val_acc: 0.7812\n",
      "Epoch 18/100\n",
      "252/252 [==============================] - 0s 340us/sample - loss: 0.5170 - acc: 0.7738 - val_loss: 0.5362 - val_acc: 0.8125\n",
      "Epoch 19/100\n",
      "252/252 [==============================] - 0s 339us/sample - loss: 0.5638 - acc: 0.7143 - val_loss: 0.4825 - val_acc: 0.7656\n",
      "Epoch 20/100\n",
      "252/252 [==============================] - 0s 333us/sample - loss: 0.4917 - acc: 0.7738 - val_loss: 0.4920 - val_acc: 0.7500\n",
      "Epoch 21/100\n",
      "252/252 [==============================] - 0s 337us/sample - loss: 0.5085 - acc: 0.7659 - val_loss: 0.4871 - val_acc: 0.7812\n",
      "Epoch 22/100\n",
      "252/252 [==============================] - 0s 342us/sample - loss: 0.5103 - acc: 0.7500 - val_loss: 0.5270 - val_acc: 0.8281\n",
      "Epoch 23/100\n",
      "252/252 [==============================] - 0s 348us/sample - loss: 0.4954 - acc: 0.8016 - val_loss: 0.5027 - val_acc: 0.7656\n",
      "Epoch 24/100\n",
      "252/252 [==============================] - 0s 343us/sample - loss: 0.5585 - acc: 0.7421 - val_loss: 0.4682 - val_acc: 0.7500\n",
      "Epoch 25/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.5324 - acc: 0.7619 - val_loss: 0.4346 - val_acc: 0.8125\n",
      "Epoch 26/100\n",
      "252/252 [==============================] - 0s 344us/sample - loss: 0.5190 - acc: 0.7302 - val_loss: 0.4717 - val_acc: 0.8125\n",
      "Epoch 27/100\n",
      "252/252 [==============================] - 0s 360us/sample - loss: 0.5216 - acc: 0.7381 - val_loss: 0.4855 - val_acc: 0.8281\n",
      "Epoch 28/100\n",
      "252/252 [==============================] - 0s 369us/sample - loss: 0.5012 - acc: 0.8016 - val_loss: 0.4724 - val_acc: 0.8281\n",
      "Epoch 29/100\n",
      "252/252 [==============================] - 0s 347us/sample - loss: 0.4291 - acc: 0.7976 - val_loss: 0.4467 - val_acc: 0.7500\n",
      "Epoch 30/100\n",
      "225/252 [=========================>....] - ETA: 0s - loss: 0.4797 - acc: 0.7867\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "252/252 [==============================] - 0s 856us/sample - loss: 0.4925 - acc: 0.7778 - val_loss: 0.4466 - val_acc: 0.8438\n",
      "Epoch 31/100\n",
      "252/252 [==============================] - 0s 350us/sample - loss: 0.4799 - acc: 0.7897 - val_loss: 0.4364 - val_acc: 0.8281\n",
      "Epoch 32/100\n",
      "252/252 [==============================] - 0s 341us/sample - loss: 0.5166 - acc: 0.7778 - val_loss: 0.4352 - val_acc: 0.8438\n",
      "Epoch 33/100\n",
      "252/252 [==============================] - 0s 342us/sample - loss: 0.4956 - acc: 0.7817 - val_loss: 0.4417 - val_acc: 0.8594\n",
      "Epoch 34/100\n",
      "252/252 [==============================] - 0s 344us/sample - loss: 0.4323 - acc: 0.8016 - val_loss: 0.4725 - val_acc: 0.8438\n",
      "Epoch 35/100\n",
      "200/252 [======================>.......] - ETA: 0s - loss: 0.4609 - acc: 0.8150\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "252/252 [==============================] - 0s 359us/sample - loss: 0.4489 - acc: 0.8056 - val_loss: 0.4448 - val_acc: 0.8594\n",
      "Epoch 36/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.4941 - acc: 0.7778 - val_loss: 0.4190 - val_acc: 0.8438\n",
      "Epoch 37/100\n",
      "252/252 [==============================] - 0s 339us/sample - loss: 0.4935 - acc: 0.7817 - val_loss: 0.4150 - val_acc: 0.8281\n",
      "Epoch 38/100\n",
      "252/252 [==============================] - 0s 336us/sample - loss: 0.4509 - acc: 0.8175 - val_loss: 0.4204 - val_acc: 0.7656\n",
      "Epoch 39/100\n",
      "252/252 [==============================] - 0s 345us/sample - loss: 0.5007 - acc: 0.7857 - val_loss: 0.4186 - val_acc: 0.8281\n",
      "Epoch 40/100\n",
      "252/252 [==============================] - 0s 344us/sample - loss: 0.4333 - acc: 0.8135 - val_loss: 0.4189 - val_acc: 0.8281\n",
      "Epoch 41/100\n",
      "252/252 [==============================] - 0s 339us/sample - loss: 0.4447 - acc: 0.7897 - val_loss: 0.4199 - val_acc: 0.8281\n",
      "Epoch 42/100\n",
      "225/252 [=========================>....] - ETA: 0s - loss: 0.4960 - acc: 0.7689\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "252/252 [==============================] - 0s 347us/sample - loss: 0.4827 - acc: 0.7738 - val_loss: 0.4242 - val_acc: 0.8438\n",
      "Epoch 43/100\n",
      "252/252 [==============================] - 0s 353us/sample - loss: 0.4128 - acc: 0.8254 - val_loss: 0.4222 - val_acc: 0.8438\n",
      "Epoch 44/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.5019 - acc: 0.7817 - val_loss: 0.4145 - val_acc: 0.8125\n",
      "Epoch 45/100\n",
      "252/252 [==============================] - 0s 336us/sample - loss: 0.4681 - acc: 0.7738 - val_loss: 0.4122 - val_acc: 0.8281\n",
      "Epoch 46/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.4484 - acc: 0.7817 - val_loss: 0.4120 - val_acc: 0.8281\n",
      "Epoch 47/100\n",
      "252/252 [==============================] - 0s 341us/sample - loss: 0.4808 - acc: 0.7937 - val_loss: 0.4110 - val_acc: 0.8281\n",
      "Epoch 48/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.4340 - acc: 0.7897 - val_loss: 0.4104 - val_acc: 0.8281\n",
      "Epoch 49/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.4377 - acc: 0.8056 - val_loss: 0.4104 - val_acc: 0.7969\n",
      "Epoch 50/100\n",
      "252/252 [==============================] - 0s 353us/sample - loss: 0.4614 - acc: 0.8056 - val_loss: 0.4097 - val_acc: 0.7969\n",
      "Epoch 51/100\n",
      "252/252 [==============================] - 0s 337us/sample - loss: 0.4400 - acc: 0.8214 - val_loss: 0.4084 - val_acc: 0.8125\n",
      "Epoch 52/100\n",
      "252/252 [==============================] - 0s 345us/sample - loss: 0.4641 - acc: 0.8016 - val_loss: 0.4075 - val_acc: 0.8281\n",
      "Epoch 53/100\n",
      "252/252 [==============================] - 0s 370us/sample - loss: 0.4259 - acc: 0.8175 - val_loss: 0.4083 - val_acc: 0.8281\n",
      "Epoch 54/100\n",
      "252/252 [==============================] - 0s 350us/sample - loss: 0.4264 - acc: 0.8135 - val_loss: 0.4084 - val_acc: 0.8281\n",
      "Epoch 55/100\n",
      "252/252 [==============================] - 0s 350us/sample - loss: 0.4576 - acc: 0.7857 - val_loss: 0.4102 - val_acc: 0.8281\n",
      "Epoch 56/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.3972 - acc: 0.8571 - val_loss: 0.4106 - val_acc: 0.8281\n",
      "Epoch 57/100\n",
      "225/252 [=========================>....] - ETA: 0s - loss: 0.4777 - acc: 0.8000\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "252/252 [==============================] - 0s 348us/sample - loss: 0.4563 - acc: 0.8095 - val_loss: 0.4094 - val_acc: 0.8281\n",
      "Epoch 58/100\n",
      "252/252 [==============================] - 0s 341us/sample - loss: 0.4427 - acc: 0.8294 - val_loss: 0.4092 - val_acc: 0.8281\n",
      "Epoch 59/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.4252 - acc: 0.8095 - val_loss: 0.4085 - val_acc: 0.8281\n",
      "Epoch 60/100\n",
      "252/252 [==============================] - 0s 352us/sample - loss: 0.4144 - acc: 0.8333 - val_loss: 0.4060 - val_acc: 0.8281\n",
      "Epoch 61/100\n",
      "252/252 [==============================] - 0s 343us/sample - loss: 0.4711 - acc: 0.8056 - val_loss: 0.4044 - val_acc: 0.8438\n",
      "Epoch 62/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.4492 - acc: 0.7976 - val_loss: 0.4037 - val_acc: 0.8438\n",
      "Epoch 63/100\n",
      "252/252 [==============================] - 0s 334us/sample - loss: 0.4129 - acc: 0.8333 - val_loss: 0.4036 - val_acc: 0.8438\n",
      "Epoch 64/100\n",
      "252/252 [==============================] - 0s 340us/sample - loss: 0.4311 - acc: 0.8294 - val_loss: 0.4035 - val_acc: 0.8125\n",
      "Epoch 65/100\n",
      "252/252 [==============================] - 0s 341us/sample - loss: 0.3783 - acc: 0.8452 - val_loss: 0.4019 - val_acc: 0.8281\n",
      "Epoch 66/100\n",
      "252/252 [==============================] - 0s 346us/sample - loss: 0.4615 - acc: 0.7817 - val_loss: 0.4003 - val_acc: 0.8438\n",
      "Epoch 67/100\n",
      "252/252 [==============================] - 0s 349us/sample - loss: 0.4542 - acc: 0.7976 - val_loss: 0.3992 - val_acc: 0.8438\n",
      "Epoch 68/100\n",
      "252/252 [==============================] - 0s 345us/sample - loss: 0.4511 - acc: 0.8373 - val_loss: 0.3985 - val_acc: 0.8438\n",
      "Epoch 69/100\n",
      "252/252 [==============================] - 0s 356us/sample - loss: 0.4728 - acc: 0.7778 - val_loss: 0.3980 - val_acc: 0.8438\n",
      "Epoch 70/100\n",
      "252/252 [==============================] - 0s 339us/sample - loss: 0.4572 - acc: 0.7778 - val_loss: 0.3974 - val_acc: 0.8438\n",
      "Epoch 71/100\n",
      "252/252 [==============================] - 0s 353us/sample - loss: 0.4286 - acc: 0.8333 - val_loss: 0.3974 - val_acc: 0.8281\n",
      "Epoch 72/100\n",
      "252/252 [==============================] - 0s 335us/sample - loss: 0.4695 - acc: 0.7897 - val_loss: 0.3968 - val_acc: 0.8438\n",
      "Epoch 73/100\n",
      "252/252 [==============================] - 0s 342us/sample - loss: 0.4218 - acc: 0.8294 - val_loss: 0.3944 - val_acc: 0.8438\n",
      "Epoch 74/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.4477 - acc: 0.8333 - val_loss: 0.3929 - val_acc: 0.8281\n",
      "Epoch 75/100\n",
      "252/252 [==============================] - 0s 331us/sample - loss: 0.4055 - acc: 0.8294 - val_loss: 0.3916 - val_acc: 0.8438\n",
      "Epoch 76/100\n",
      "252/252 [==============================] - 0s 350us/sample - loss: 0.4464 - acc: 0.8254 - val_loss: 0.3920 - val_acc: 0.8438\n",
      "Epoch 77/100\n",
      "252/252 [==============================] - 0s 338us/sample - loss: 0.4864 - acc: 0.7579 - val_loss: 0.3914 - val_acc: 0.8438\n",
      "Epoch 78/100\n",
      "252/252 [==============================] - 0s 337us/sample - loss: 0.4720 - acc: 0.8095 - val_loss: 0.3945 - val_acc: 0.8125\n",
      "Epoch 79/100\n",
      "252/252 [==============================] - 0s 340us/sample - loss: 0.4048 - acc: 0.8254 - val_loss: 0.3932 - val_acc: 0.8125\n",
      "Epoch 80/100\n",
      "252/252 [==============================] - 0s 370us/sample - loss: 0.4041 - acc: 0.8135 - val_loss: 0.3945 - val_acc: 0.8125\n",
      "Epoch 81/100\n",
      "252/252 [==============================] - 0s 353us/sample - loss: 0.4544 - acc: 0.8135 - val_loss: 0.3951 - val_acc: 0.8125\n",
      "Epoch 82/100\n",
      "225/252 [=========================>....] - ETA: 0s - loss: 0.4070 - acc: 0.8311\n",
      "Epoch 00082: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "252/252 [==============================] - 0s 342us/sample - loss: 0.4012 - acc: 0.8373 - val_loss: 0.3957 - val_acc: 0.8125\n",
      "Epoch 83/100\n",
      "252/252 [==============================] - 0s 336us/sample - loss: 0.4268 - acc: 0.8016 - val_loss: 0.3958 - val_acc: 0.7969\n",
      "Epoch 84/100\n",
      "252/252 [==============================] - 0s 347us/sample - loss: 0.4164 - acc: 0.8175 - val_loss: 0.3970 - val_acc: 0.7812\n",
      "Epoch 85/100\n",
      "252/252 [==============================] - 0s 344us/sample - loss: 0.4651 - acc: 0.8056 - val_loss: 0.3979 - val_acc: 0.7812\n",
      "Epoch 86/100\n",
      "252/252 [==============================] - 0s 343us/sample - loss: 0.3881 - acc: 0.8373 - val_loss: 0.3969 - val_acc: 0.7812\n",
      "Epoch 87/100\n",
      "225/252 [=========================>....] - ETA: 0s - loss: 0.4246 - acc: 0.7822\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "252/252 [==============================] - 0s 348us/sample - loss: 0.4274 - acc: 0.7817 - val_loss: 0.3973 - val_acc: 0.7812\n",
      "Epoch 88/100\n",
      "252/252 [==============================] - 0s 348us/sample - loss: 0.4078 - acc: 0.8056 - val_loss: 0.3974 - val_acc: 0.7812\n",
      "Epoch 89/100\n",
      "252/252 [==============================] - 0s 378us/sample - loss: 0.4152 - acc: 0.8135 - val_loss: 0.3970 - val_acc: 0.7812\n",
      "Epoch 90/100\n",
      "252/252 [==============================] - 0s 355us/sample - loss: 0.4127 - acc: 0.8254 - val_loss: 0.3958 - val_acc: 0.7812\n",
      "Epoch 91/100\n",
      "252/252 [==============================] - 0s 402us/sample - loss: 0.4913 - acc: 0.7698 - val_loss: 0.3947 - val_acc: 0.8281\n",
      "Epoch 92/100\n",
      "200/252 [======================>.......] - ETA: 0s - loss: 0.4434 - acc: 0.7750\n",
      "Epoch 00092: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "252/252 [==============================] - 0s 369us/sample - loss: 0.4279 - acc: 0.7857 - val_loss: 0.3938 - val_acc: 0.8281\n",
      "Epoch 93/100\n",
      "252/252 [==============================] - 0s 368us/sample - loss: 0.4532 - acc: 0.8175 - val_loss: 0.3941 - val_acc: 0.8125\n",
      "Epoch 94/100\n",
      "252/252 [==============================] - 0s 358us/sample - loss: 0.4115 - acc: 0.8095 - val_loss: 0.3937 - val_acc: 0.8281\n",
      "Epoch 95/100\n",
      "252/252 [==============================] - 0s 385us/sample - loss: 0.4405 - acc: 0.8294 - val_loss: 0.3938 - val_acc: 0.8281\n",
      "Epoch 96/100\n",
      "252/252 [==============================] - 0s 391us/sample - loss: 0.3972 - acc: 0.8214 - val_loss: 0.3937 - val_acc: 0.8125\n",
      "Epoch 97/100\n",
      "200/252 [======================>.......] - ETA: 0s - loss: 0.4025 - acc: 0.8450\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "252/252 [==============================] - 0s 386us/sample - loss: 0.4148 - acc: 0.8254 - val_loss: 0.3938 - val_acc: 0.8125\n",
      "Epoch 98/100\n",
      "252/252 [==============================] - 0s 343us/sample - loss: 0.4463 - acc: 0.8254 - val_loss: 0.3940 - val_acc: 0.8125\n",
      "Epoch 99/100\n",
      "252/252 [==============================] - 0s 343us/sample - loss: 0.4286 - acc: 0.7976 - val_loss: 0.3940 - val_acc: 0.8125\n",
      "Epoch 100/100\n",
      "252/252 [==============================] - 0s 340us/sample - loss: 0.4428 - acc: 0.8095 - val_loss: 0.3939 - val_acc: 0.8125\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  NC vs. AD - dimension reduction \"\"\"\n",
    "\n",
    "# create input for binary classification of NC vs. AD\n",
    "Xtr_ncad, ytr_ncad = data_filter(Xtr_dr, ytr, 1)\n",
    "Xval_ncad, yval_ncad = data_filter(Xval_dr, yval, 1)\n",
    "Xts_ncad, yts_ncad = data_filter(Xts_dr, yts, 1)\n",
    "\n",
    "# rename the input\n",
    "X_train = Xtr_ncad\n",
    "X_test = Xts_ncad\n",
    "X_val = Xval_ncad\n",
    "\n",
    "# one hot encode the target labels \n",
    "y_train = onehot_bi(ytr_ncad)\n",
    "y_test = onehot_bi(yts_ncad)\n",
    "y_val = onehot_bi(yval_ncad)\n",
    "\n",
    "\n",
    "# model training\n",
    "model, hist = run_adpbase(X_train, y_train, X_val, y_val, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydot\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ea/76/75b1bb82e9bad3e3d656556eaa353d8cd17c4254393b08ec9786ac8ed273/pydot-1.4.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from pydot)\n",
      "Installing collected packages: pydot\n",
      "Successfully installed pydot-1.4.2\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting pydotplus\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/60/bf/62567830b700d9f6930e9ab6831d6ba256f7b0b730acb37278b0ccdffacf/pydotplus-2.0.2.tar.gz (278kB)\n",
      "\u001b[K    100% || 286kB 24.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages (from pydotplus)\n",
      "Building wheels for collected packages: pydotplus\n",
      "  Running setup.py bdist_wheel for pydotplus ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ma-user/.cache/pip/wheels/ad/d2/dc/4b6affd6cb0f0c43f30fb56830bcbb807b2b3a484990a9f0d5\n",
      "Successfully built pydotplus\n",
      "Installing collected packages: pydotplus\n",
      "Successfully installed pydotplus-2.0.2\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: graphviz in /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 22.0.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We finish building the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(3, 256, 2..., padding=\"same\", data_format=\"channels_first\")`\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", data_format=\"channels_first\")`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Convolution2D, Flatten, Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD , Adam\n",
    "# from keras.initializations import normal\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "import pydot as pyd\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "keras.utils.vis_utils.pydot = pyd\n",
    "\n",
    "\n",
    "# apply a 3x3 convolution with 64 output filters on a 256x256 image:\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', dim_ordering='th',input_shape=(3, 256, 256)))\n",
    "# now model.output_shape == (None, 64, 256, 256)\n",
    "\n",
    "# add a 3x3 convolution on top, with 32 output filters:\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same', dim_ordering='th'))\n",
    "# now model.output_shape == (None, 32, 256, 256)\n",
    "adam = Adam(lr=1e-6)\n",
    "model.compile(loss='mse',optimizer=adam)\n",
    "print(\"We finish building the model\")\n",
    "\n",
    "plot_model(model, to_file='test1.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.13.1",
   "language": "python",
   "name": "tensorflow-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
