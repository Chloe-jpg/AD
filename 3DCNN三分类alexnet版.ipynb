{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from imblearn.metrics import sensitivity_specificity_support as sss\n",
    "from sklearn.metrics import  f1_score, precision_score, recall_score, accuracy_score, roc_curve, auc, roc_auc_score, confusion_matrix as CM\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, Conv3D, GlobalAveragePooling3D,  MaxPooling3D, LeakyReLU, BatchNormalization, Dropout, Flatten, Activation, Reshape,  Conv3DTranspose, UpSampling3D\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cretae the binary input \n",
    "def data_filter(data, label, exclu):\n",
    "    idx = np.where(label!= exclu)[0]\n",
    "    #print(len(idx))\n",
    "    data_new = data[idx]\n",
    "    label_new = label[idx]\n",
    "    #print(data_new.shape)\n",
    "    print(np.unique(label_new, return_counts=True))\n",
    "    return data_new, label_new\n",
    "\n",
    "# onehot encode labels for binary classifications\n",
    "def onehot_bi(y):\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    y = y.reshape(len(y), 1)\n",
    "    y_encoded = onehot_encoder.fit_transform(y)\n",
    "    return y_encoded\n",
    "\n",
    "# onehot encode labels for 3-way classifications\n",
    "def onehot_tri(y):\n",
    "    from keras.utils import to_categorical\n",
    "    return to_categorical(y)\n",
    "\n",
    "# view the distribution of class labels of the input data\n",
    "def showpercentage(array):\n",
    "    pcn = array[1][0]/np.sum(array[1])\n",
    "    pmci = array[1][1]/np.sum(array[1])\n",
    "    pad = array[1][2]/np.sum(array[1])\n",
    "    print(str(pad) + \" percent of the data has AD label\")\n",
    "    print(str(pcn) + \" percent of the data has CN label\")\n",
    "    print(str(pmci) + \" percent of the data has MCI label\")\n",
    "# visualizatio of model traning and model performance \n",
    "\n",
    "# visualize the training and validation performance\n",
    "def plot_history(data_list, label_list, title, ylabel, name):\n",
    "\n",
    "    epochs = range(1, len(data_list[0]) + 1)\n",
    "\n",
    "    for data, label in zip(data_list, label_list):\n",
    "        plt.plot(epochs, data, label=label)\n",
    "    plt.title(title, pad = 10, fontsize='large')\n",
    "    plt.xlabel('Epochs', labelpad=10)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "#%%\n",
    "\n",
    "# model evaluation\n",
    "    \n",
    "# evaluate model performance - binary classifications\n",
    "def evaluate_binary(X_test, y_test, model, name):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        test_y_prob = model.predict(X_test)\n",
    "    print(test_y_prob)\n",
    "    test_y_pred = np.argmax(test_y_prob, axis = 1)\n",
    "    test_y_true = np.argmax(y_test, axis = 1) \n",
    "    # accuracy\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss, acc = model.evaluate(X_test, y_test)\n",
    "    # AUC\n",
    "    pos_prob = test_y_prob[:,1]\n",
    "    auc_score = roc_auc_score(test_y_true, pos_prob)\n",
    "    # precision, recall, specificity, and f1_score\n",
    "    p = precision_score(test_y_true, test_y_pred)\n",
    "    r = recall_score(test_y_true, test_y_pred)\n",
    "    f1 = f1_score(test_y_true, test_y_pred)\n",
    "#     sen, spe, _ = sss(test_y_true, test_y_pred, average=\"binary\")\n",
    "    print(test_y_true, test_y_pred)\n",
    "    # print results\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    print(\"Test AUC is: \", auc_score)\n",
    "    print(\"Test confusion matrix: \\n\", CM(test_y_true, test_y_pred))\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "#     print(\"Specificity: \", spe)\n",
    "    print(\"f1_score: \", f1)\n",
    "\n",
    "    # plot and save roc curve\n",
    "    pos_prob = test_y_prob[:,1]\n",
    "    fpr, tpr, thresholds = roc_curve(test_y_true, pos_prob)\n",
    "    ns_probs = [0 for _ in range(len(test_y_prob))]\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(test_y_true, ns_probs)\n",
    "    plt.axis([0,1,0,1]) \n",
    "    plt.plot(fpr,tpr, marker = '.', color = 'darkorange', label = 'Model AUC (area = {:.2f})'.format(auc_score)) \n",
    "    plt.plot(ns_fpr, ns_tpr, color = 'royalblue', linestyle='--')\n",
    "    plt.legend()\n",
    "    plt.xlabel('False Positive Rate') \n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.savefig(name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "# evaluate model performance - 3 way classifiations\n",
    "def evaluate_3way(X_test, y_test, model):\n",
    "    test_y_prob = model.predict(X_test)\n",
    "    test_y_pred = np.argmax(test_y_prob, axis = 1)\n",
    "    test_y_true = np.argmax(y_test, axis = 1) \n",
    "    print(test_y_prob)\n",
    "    # accuracy\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    # precision, recall, specificity, and f1_score\n",
    "    p = precision_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "    r = recall_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "    f1 = f1_score(test_y_true, test_y_pred, average=\"macro\")\n",
    "#     sen,spe,_ = sss(test_y_true, test_y_pred, average=\"macro\")\n",
    "    print(test_y_true, test_y_pred)\n",
    "    print(\"Test accuracy:\", acc)\n",
    "    print(\"Test confusion matrix: \\n\", CM(test_y_true, test_y_pred))\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "#     print(\"Specificity: \", spe)\n",
    "    print(\"f1_score: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356, 64, 64, 64)\n",
      "(356,)\n",
      "(86, 64, 64, 64)\n",
      "(86,)\n",
      "(90, 64, 64, 64)\n",
      "(90,)\n",
      "Train:\n",
      "0.2893258426966292 percent of the data has AD label\n",
      "0.41853932584269665 percent of the data has CN label\n",
      "0.29213483146067415 percent of the data has MCI label\n",
      "\n",
      "Validation:\n",
      "0.28888888888888886 percent of the data has AD label\n",
      "0.4222222222222222 percent of the data has CN label\n",
      "0.28888888888888886 percent of the data has MCI label\n",
      "\n",
      "Test\n",
      "0.29069767441860467 percent of the data has AD label\n",
      "0.4186046511627907 percent of the data has CN label\n",
      "0.29069767441860467 percent of the data has MCI label\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  Load in the input - original data \"\"\"\n",
    "\n",
    "Xtr = np.load(\"preprocess/input/random_split/train_data.npy\", allow_pickle = True)\n",
    "ytr = np.load(\"preprocess/input/random_split/train_label.npy\", allow_pickle = True)\n",
    "print(Xtr.shape)\n",
    "print(ytr.shape)\n",
    "\n",
    "Xts = np.load(\"preprocess/input/random_split/test_data.npy\", allow_pickle = True)\n",
    "yts = np.load(\"preprocess/input/random_split/test_label.npy\", allow_pickle = True)\n",
    "print(Xts.shape)\n",
    "print(yts.shape)\n",
    "\n",
    "Xval = np.load(\"preprocess/input/random_split/val_data.npy\", allow_pickle = True)\n",
    "yval = np.load(\"preprocess/input/random_split/val_label.npy\", allow_pickle = True)\n",
    "print(Xval.shape)\n",
    "print(yval.shape)\n",
    "\n",
    "print(\"Train:\")\n",
    "showpercentage(np.unique(ytr, return_counts=True))\n",
    "print()\n",
    "print(\"Validation:\")\n",
    "showpercentage(np.unique(yval, return_counts=True))\n",
    "print()\n",
    "print(\"Test\")\n",
    "showpercentage(np.unique(yts, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the baseline model\n",
    "def run_alexnet(X_train, y_train, X_valid = None, y_valid = None, \n",
    "             final = False, out = 2,\n",
    "             dr = 0.02, lr = 0.00001, \n",
    "             breg = l2(0.0001), areg = None, \n",
    "             n_epochs = 50, batch_size = 15):\n",
    "  \n",
    "    dim = (64, 64, 64, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32, kernel_size=(11,11,11),  kernel_initializer='he_uniform', bias_regularizer=breg, input_shape=dim))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(Conv3D(64, kernel_size=(5,5,5),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(1,1,1)))\n",
    "    \n",
    "    model.add(Conv3D(128, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(1,1,1)))\n",
    "    \n",
    "    model.add(Conv3D(128, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(1,1,1)))\n",
    "    \n",
    "    \n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Dense(256, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(out, activation='softmax', activity_regularizer=areg))\n",
    "\n",
    "    # model optimization\n",
    "    opt = Adam(lr = lr)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.5, patience = 5, \n",
    "                         verbose = 1, epsilon = 1e-4, mode = 'min')\n",
    "    \n",
    "  # model training and fine-tuning\n",
    "    if not final:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                     batch_size = batch_size, \n",
    "                     epochs = n_epochs,\n",
    "                     callbacks=[cb],\n",
    "                     validation_data = (X_valid, y_valid), \n",
    "                     shuffle = True)\n",
    "  \n",
    "  # model final training for testing (train + valid combined)\n",
    "    else:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                  batch_size = batch_size, \n",
    "                  epochs = n_epochs,\n",
    "                  callbacks=[cb],\n",
    "                  shuffle = True)\n",
    "\n",
    "\n",
    "    return model, hist\n",
    "# build the baseline model\n",
    "def run_voxcnn(X_train, y_train, X_valid = None, y_valid = None, \n",
    "             final = False, out = 2,\n",
    "             dr = 0.02, lr = 0.00001, \n",
    "             breg = l2(0.0001), areg = None, \n",
    "             n_epochs = 50, batch_size = 15):\n",
    "  \n",
    "    dim = (64, 64, 64, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32, kernel_size=(3,3,3),  kernel_initializer='he_uniform', bias_regularizer=breg, input_shape=dim))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv3D(32, kernel_size=(3,3,3),  kernel_initializer='he_uniform', bias_regularizer=breg, input_shape=dim))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(Conv3D(64, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv3D(64, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv3D(128, kernel_size=(3,3,3),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Dense(256, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(out, activation='softmax', activity_regularizer=areg))\n",
    "\n",
    "    # model optimization\n",
    "    opt = Adam(lr = lr)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.5, patience = 5, \n",
    "                         verbose = 1, epsilon = 1e-4, mode = 'min')\n",
    "    \n",
    "  # model training and fine-tuning\n",
    "    if not final:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                     batch_size = batch_size, \n",
    "                     epochs = n_epochs,\n",
    "                     callbacks=[cb],\n",
    "                     validation_data = (X_valid, y_valid), \n",
    "                     shuffle = True)\n",
    "  \n",
    "  # model final training for testing (train + valid combined)\n",
    "    else:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                  batch_size = batch_size, \n",
    "                  epochs = n_epochs,\n",
    "                  callbacks=[cb],\n",
    "                  shuffle = True)\n",
    "\n",
    "\n",
    "    return model, hist\n",
    "\n",
    "# build the baseline model\n",
    "def run_base(X_train, y_train, X_valid = None, y_valid = None, \n",
    "             final = False, out = 2,\n",
    "             dr = 0.02, lr = 0.00001, \n",
    "             breg = l2(0.0001), areg = None, \n",
    "             n_epochs = 30, batch_size = 15):\n",
    "  \n",
    "    dim = (64, 64, 64, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(32, kernel_size=(5,5,5),  kernel_initializer='he_uniform', bias_regularizer=breg, input_shape=dim))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "    model.add(Conv3D(64, kernel_size=(5,5,5),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "\n",
    "    model.add(Conv3D(128, kernel_size=(5,5,5),  bias_regularizer=breg, kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dropout(dr))\n",
    "\n",
    "    model.add(Dense(256, bias_regularizer=breg,   kernel_initializer='he_uniform'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(out, activation='softmax', activity_regularizer=areg))\n",
    "\n",
    "    # model optimization\n",
    "    opt = Adam(lr = lr)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "    cb = ReduceLROnPlateau(monitor = 'val_loss', \n",
    "                         factor = 0.5, patience = 5, \n",
    "                         verbose = 1, epsilon = 1e-4, mode = 'min')\n",
    "    \n",
    "  # model training and fine-tuning\n",
    "    if not final:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                     batch_size = batch_size, \n",
    "                     epochs = n_epochs,\n",
    "                     callbacks=[cb],\n",
    "                     validation_data = (X_valid, y_valid), \n",
    "                     shuffle = True)\n",
    "  \n",
    "  # model final training for testing (train + valid combined)\n",
    "    else:\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                  batch_size = batch_size, \n",
    "                  epochs = n_epochs,\n",
    "                  callbacks=[cb],\n",
    "                  shuffle = True)\n",
    "\n",
    "\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([149, 104]))\n",
      "(array([0, 1]), array([38, 26]))\n",
      "(array([0, 1]), array([36, 25]))\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "Train on 253 samples, validate on 64 samples\n",
      "WARNING:tensorflow:From /home/ma-user/anaconda3/envs/TensorFlow-1.13.1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "253/253 [==============================] - 7s 28ms/sample - loss: 0.6571 - acc: 0.6166 - val_loss: 0.6395 - val_acc: 0.6875\n",
      "Epoch 2/50\n",
      "253/253 [==============================] - 2s 9ms/sample - loss: 0.6100 - acc: 0.6719 - val_loss: 0.6518 - val_acc: 0.5938\n",
      "Epoch 3/50\n",
      "253/253 [==============================] - 2s 9ms/sample - loss: 0.5528 - acc: 0.7628 - val_loss: 0.5928 - val_acc: 0.7500\n",
      "Epoch 4/50\n",
      " 90/253 [=========>....................] - ETA: 1s - loss: 0.4884 - acc: 0.8222"
     ]
    }
   ],
   "source": [
    "\"\"\"  Baseline NC vs. MCI \"\"\"\n",
    "\n",
    "# create input for binary classification of NC vs. MCI\n",
    "Xtr_ncmci, ytr_ncmci = data_filter(Xtr, ytr, 2)\n",
    "Xval_ncmci, yval_ncmci = data_filter(Xval, yval, 2)\n",
    "Xts_ncmci, yts_ncmci = data_filter(Xts, yts, 2)\n",
    "\n",
    "# reshape the input\n",
    "X_train = Xtr_ncmci.reshape(-1,64,64,64,1) \n",
    "X_test = Xts_ncmci.reshape(-1,64,64,64,1) \n",
    "X_val = Xval_ncmci.reshape(-1,64,64,64,1) \n",
    "\n",
    "# one hot encode the target labels \n",
    "y_train = onehot_bi(ytr_ncmci)\n",
    "y_test = onehot_bi(yts_ncmci)\n",
    "y_val = onehot_bi(yval_ncmci)\n",
    "\n",
    "\n",
    "# model training\n",
    "model, hist = run_alexnet(X_train, y_train, X_val, y_val, areg = l1(0.001))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.13.1",
   "language": "python",
   "name": "tensorflow-1.13.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
